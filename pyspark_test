import pyspark.sql.functions as F
from pyspark import SparkContext, SparkConf
from pyspark.sql import *
from pyspark.sql.functions import max
from pyspark.sql.functions import min 
from pyspark.sql.functions import avg
from pyspark.sql.functions import desc 
from pyspark.sql.functions import window, column, desc, col
from pyspark.sql.types import *
from pyspark.sql.types import StructField, StructType, StringType, LongType
from pyspark.ml.feature import Imputer, Binarizer
import pandas as pd
from datetime import datetime, timedelta


spark = (SparkSession
                .builder
                .appName('data_query_theo')
                .getOrCreate())
#csv_read                
data_pega = '/user/hdp_dev/test/datates.csv'
data = spark.read.option("inferSchema","true").option("header","true").csv(data_pega)


#staticData
staticDataFrame = spark.read.format("csv")\
.option("header", "true")\
.option("inferSchema", "true")\
.load("/user/hdp_dev/test/daily_data/*.csv")

staticDataFrame.createOrReplaceTempView("retail_data")
staticSchema = staticDataFrame.schema


staticDataFrame\
.selectExpr(
"CustomerId",
"(UnitPrice * Quantity) as total_cost",
"InvoiceDate")\
.groupBy(
col("CustomerId"), window(col("InvoiceDate"), "1 day"))\
.sum("total_cost")\
.show(5)



#streamingData
streamingDataFrame = spark.readStream\
.schema(staticSchema)\
.option("maxFilesPerTrigger", 1)\
.format("csv")\
.option("header", "true")\
.load("/user/hdp_dev/test/daily_data/*.csv")

streamingDataFrame.isStreaming 

purchaseByCustomerPerHour = streamingDataFrame\
.selectExpr(
"CustomerId",
"(UnitPrice * Quantity) as total_cost",
"InvoiceDate")\
.groupBy(
col("CustomerId"), window(col("InvoiceDate"), "1 day"))\
.sum("total_cost")

purchaseByCustomerPerHour.writeStream\
.format("memory")\
.queryName("customer_buy")\
.outputMode("complete")\
.start()

spark.sql("""
SELECT *
FROM customer_buy
""")\
.show(5)


df.first()

myManualSchema = StructType([
StructField("DEST_COUNTRY_NAME", StringType(), True),
StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
StructField("count", LongType(), False, metadata={"hello":"world"})
])

df = spark.read.format("json").schema(myManualSchema).load("/user/hdp_dev/test/daily_data/2015-summary.json").show()



#ROW

myRow = Row("Hello", None, 1, False)
myRow[0],myRow[1],myRow[3]

#TEMP_VIEW
df = spark.read.format("json").load("/user/hdp_dev/test/daily_data/2015-summary.json")
df.createOrReplaceTempView("dfTable")


#DDL_COLUMN
df = spark.read.format("json").load("/user/hdp_dev/test/daily_data/2015-summary.json")
df.selectExpr("*","(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME)as Negara")\
.show(2)

df.withColumn("Col_1",expr("DEST_COUNTRY_NAME=ORIGIN_COUNTRY_NAME"))

nameCol = df.withColumnRenamed("DEST_COUNTRY_NAME", "dest").show(2)
